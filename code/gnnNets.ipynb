{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b29262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from typing import Union, List\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "from torch_geometric.nn.glob import global_mean_pool, global_add_pool, global_max_pool\n",
    "from torch import Tensor\n",
    "from torch_sparse import SparseTensor, fill_diag\n",
    "from torch_geometric.typing import Adj, OptTensor, Size\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "\n",
    "def get_gnnNets(input_dim, output_dim, gcn_model_params):\n",
    "    return GCNNet(input_dim=input_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      ** gcn_model_params)\n",
    "\n",
    "\n",
    "def identity(x: torch.Tensor, batch: torch.Tensor):\n",
    "    return x\n",
    "\n",
    "\n",
    "def cat_max_sum(x, batch):\n",
    "    node_dim = x.shape[-1]\n",
    "    num_node = 25\n",
    "    x = x.reshape(-1, num_node, node_dim)\n",
    "    return torch.cat([x.max(dim=1)[0], x.sum(dim=1)], dim=-1)\n",
    "\n",
    "\n",
    "def get_readout_layers(readout):\n",
    "    readout_func_dict = {\n",
    "        \"mean\": global_mean_pool,\n",
    "        \"sum\": global_add_pool,\n",
    "        \"max\": global_max_pool,\n",
    "        'identity': identity,\n",
    "        \"cat_max_sum\": cat_max_sum,\n",
    "    }\n",
    "    readout_func_dict = {k.lower(): v for k, v in readout_func_dict.items()}\n",
    "    return readout_func_dict[readout.lower()]\n",
    "\n",
    "\n",
    "# GNN_LRP takes GNNPool class as pooling layer\n",
    "class GNNPool(nn.Module):\n",
    "    def __init__(self, readout):\n",
    "        super().__init__()\n",
    "        self.readout = get_readout_layers(readout)\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return self.readout(x, batch)\n",
    "    \n",
    "\n",
    "def get_nonlinear(nonlinear):\n",
    "    nonlinear_func_dict = {\n",
    "        \"relu\": F.relu,\n",
    "        \"leakyrelu\": partial(F.leaky_relu, negative_slope=0.2),\n",
    "        \"sigmoid\": F.sigmoid,\n",
    "        \"elu\": F.elu\n",
    "    }\n",
    "    return nonlinear_func_dict[nonlinear]\n",
    "\n",
    "\"\"\"\n",
    "github의 1.0.0버전의 것은 자연스럽지 않다. benchmark에 있는 것이 model(Data)식으로 사용 가능\n",
    "edge_attr도 다루도록 수정\n",
    "\"\"\"\n",
    "class GNNBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def arguments_read(self, *args, **kwargs):\n",
    "        if args:\n",
    "            if len(args) == 1:\n",
    "                data = args[0]\n",
    "                x = data.x\n",
    "                edge_index = data.edge_index\n",
    "                edge_attr = data.edge_attr\n",
    "                if hasattr(data, 'batch'):\n",
    "                    batch = data.batch\n",
    "                else:\n",
    "                    batch = torch.zeros(x.shape[0], dtype=torch.int64, device=x.device)\n",
    "\n",
    "            elif len(args) == 3:\n",
    "                x, edge_index, edge_attr = args[0], args[1], args[2]\n",
    "                batch = torch.zeros(x.shape[0], dtype=torch.int64, device=x.device)\n",
    "\n",
    "            elif len(args) == 4:\n",
    "                x, edge_index, edge_attr, batch = args[0], args[1], args[2], args[3]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"forward's args should take 1, 3 or 4 arguments but got {len(args)}\")\n",
    "        else:\n",
    "            data: Batch = kwargs.get('data')\n",
    "            if not data:\n",
    "                x = kwargs.get('x')\n",
    "                edge_index = kwargs.get('edge_index')\n",
    "                edge_attr = kwargs.get('edge_attr')\n",
    "                assert x is not None, \"forward's args is empty and required node features x is not in kwargs\"\n",
    "                assert edge_index is not None, \"forward's args is empty and required edge_index is not in kwargs\"\n",
    "                \n",
    "                batch = kwargs.get('batch')\n",
    "                if not batch:\n",
    "                    batch = torch.zeros(x.shape[0], dtype=torch.int64, device=x.device)\n",
    "            else:\n",
    "                x = data.x\n",
    "                edge_index = data.edge_index\n",
    "                edge_attr = data.edge_attr\n",
    "                if hasattr(data, 'batch'):\n",
    "                    batch = data.batch\n",
    "                else:\n",
    "                    batch = torch.zeros(x.shape[0], dtype=torch.int64, device=x.device)\n",
    "        return x, edge_index, edge_attr, batch\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "기존 GCNConv에서 s = self.lin(x)하던 것을 weight를 self.weight로 옮기고 matmul로 직접 해줬다. 왜?\n",
    "\n",
    "AX가 X\\Theta\\보다 먼저 곱해지게 순서가 바뀌었는데, 딱히 상관은 없을듯\n",
    "\n",
    "edge_weight가 none이라 가정하고 forward()가 동작한다.\n",
    "입력으로 받도록 기존 PyG방식으로 수정함.\n",
    "\n",
    "다음 attribution들은 버전 오류로 보임.\n",
    "check_input, collect, edge_mask, fused_user_args, user_args\n",
    "일단 __{}__을 _{}로 수정\n",
    "\n",
    "propagate는 PyG 2.0.0에 비해 hook이 사라진 것 말고는 차이가 없는듯. 왜 다시 정의했지?\n",
    "\"\"\"\n",
    "# GCNConv\n",
    "class GCNConv(GCNConv):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(GCNConv, self).__init__(*args, **kwargs)\n",
    "        self.weight = nn.Parameter(self.lin.weight.data.T.clone().detach())\n",
    "\n",
    "    # remove the sigmoid operation for edge_mask in the propagation method\n",
    "    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n",
    "        size = self._check_input(edge_index, size)\n",
    "\n",
    "        # Run \"fused\" message and aggregation (if applicable).\n",
    "        if (isinstance(edge_index, SparseTensor) and self.fuse\n",
    "                and not self.__explain__):\n",
    "            coll_dict = self._collect(self._fused_user_args, edge_index,\n",
    "                                         size, kwargs)\n",
    "\n",
    "            msg_aggr_kwargs = self.inspector.distribute(\n",
    "                'message_and_aggregate', coll_dict)\n",
    "            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n",
    "\n",
    "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
    "            return self.update(out, **update_kwargs)\n",
    "\n",
    "        # Otherwise, run both functions in separation.\n",
    "        elif isinstance(edge_index, Tensor) or not self.fuse:\n",
    "            coll_dict = self._collect(self._user_args, edge_index, size,\n",
    "                                         kwargs)\n",
    "\n",
    "            msg_kwargs = self.inspector.distribute('message', coll_dict)\n",
    "            out = self.message(**msg_kwargs)\n",
    "\n",
    "            # For `GNNExplainer`, we require a separate message and aggregate\n",
    "            # procedure since this allows us to inject the `edge_mask` into the\n",
    "            # message passing computation scheme.\n",
    "            if self._explain:\n",
    "                edge_mask = self._edge_mask\n",
    "                # Some ops add self-loops to `edge_index`. We need to do the\n",
    "                # same for `edge_mask` (but do not train those).\n",
    "                if out.size(self.node_dim) != edge_mask.size(0):\n",
    "                    loop = edge_mask.new_ones(size[0])\n",
    "                    edge_mask = torch.cat([edge_mask, loop], dim=0)\n",
    "                assert out.size(self.node_dim) == edge_mask.size(0), 'adding self loop for explain makes different dim'\n",
    "                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n",
    "\n",
    "            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n",
    "            out = self.aggregate(out, **aggr_kwargs)\n",
    "\n",
    "            update_kwargs = self.inspector.distribute('update', coll_dict)\n",
    "            return self.update(out, **update_kwargs)\n",
    "\n",
    "    # add edge_weight for normalize=False\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(   # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops, dtype=x.dtype)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        # --- add require_grad ---\n",
    "        edge_weight.requires_grad_(True)\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                             size=None)\n",
    "        \n",
    "        \n",
    "        out = torch.matmul(out, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        # --- My: record edge_weight ---\n",
    "        self.edge_weight = edge_weight\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "get_emb, forward에서 edge_attr를 사용하도록 수정\n",
    "\"\"\"\n",
    "class GCNNet(GNNBase):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 gnn_latent_dim: Union[List[int]],\n",
    "                 gnn_dropout: float = 0.0,\n",
    "                 gnn_emb_normalization: bool = False,\n",
    "                 gcn_adj_normalization: bool = True,\n",
    "                 add_self_loop: bool = True,\n",
    "                 gnn_nonlinear: str = 'relu',\n",
    "                 readout: str = 'max',\n",
    "                 concate: bool = False,\n",
    "                 fc_latent_dim: Union[List[int]] = [],\n",
    "                 fc_dropout: float = 0.0,\n",
    "                 fc_nonlinear: str = 'relu',\n",
    "                 ):\n",
    "        super(GCNNet, self).__init__()\n",
    "        # first and last layer - dim_features and classes\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        # GNN part\n",
    "        self.gnn_latent_dim = gnn_latent_dim\n",
    "        self.gnn_dropout = gnn_dropout\n",
    "        self.num_gnn_layers = len(self.gnn_latent_dim)\n",
    "        self.add_self_loop = add_self_loop\n",
    "        self.gnn_emb_normalization = gnn_emb_normalization\n",
    "        self.gcn_adj_normalization = gcn_adj_normalization\n",
    "        self.gnn_nonlinear = get_nonlinear(gnn_nonlinear)\n",
    "        self.concate = concate\n",
    "        # readout\n",
    "        self.readout_layer = GNNPool(readout)\n",
    "        # FC part\n",
    "        self.fc_latent_dim = fc_latent_dim\n",
    "        self.fc_dropout = fc_dropout\n",
    "        self.num_mlp_layers = len(self.fc_latent_dim) + 1\n",
    "        self.fc_nonlinear = get_nonlinear(fc_nonlinear)\n",
    "\n",
    "        if self.concate:\n",
    "            self.emb_dim = sum(self.gnn_latent_dim)\n",
    "        else:\n",
    "            self.emb_dim = self.gnn_latent_dim[-1]\n",
    "\n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, self.gnn_latent_dim[0],\n",
    "                                  add_self_loops=self.add_self_loop,\n",
    "                                  normalize=self.gcn_adj_normalization))\n",
    "        for i in range(1, self.num_gnn_layers):\n",
    "            self.convs.append(GCNConv(self.gnn_latent_dim[i - 1], self.gnn_latent_dim[i],\n",
    "                                      add_self_loops=self.add_self_loop,\n",
    "                                      normalize=self.gcn_adj_normalization))\n",
    "        # FC layers\n",
    "        self.mlps = nn.ModuleList()\n",
    "        if self.num_mlp_layers > 1:\n",
    "            self.mlps.append(nn.Linear(self.emb_dim, self.fc_latent_dim[0]))\n",
    "\n",
    "            for i in range(1, self.num_mlp_layers-1):\n",
    "                self.mlps.append(nn.Linear(self.fc_latent_dim[i-1], self.fc_latent_dim[1]))\n",
    "            self.mlps.append(nn.Linear(self.fc_latent_dim[-1], self.output_dim))\n",
    "        else:\n",
    "            self.mlps.append(nn.Linear(self.emb_dim, self.output_dim))\n",
    "\n",
    "    def device(self):\n",
    "        return self.convs[0].weight.device\n",
    "\n",
    "    def get_emb(self, *args, **kwargs):\n",
    "        #  node embedding for GNN\n",
    "        x, edge_index, edge_attr, batch = self.arguments_read(*args, **kwargs)\n",
    "        xs = []\n",
    "        for i in range(self.num_gnn_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            if self.gnn_emb_normalization:\n",
    "                x = F.normalize(x, p=2, dim=-1)\n",
    "            x = self.gnn_nonlinear(x)\n",
    "            x = F.dropout(x, self.gnn_dropout)\n",
    "            xs.append(x)\n",
    "\n",
    "        if self.concate:\n",
    "            return torch.cat(xs, dim=1)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        _, _, _, batch = self.arguments_read(*args, **kwargs)\n",
    "        # node embedding for GNN\n",
    "        emb = self.get_emb(*args, **kwargs)\n",
    "        # pooling process\n",
    "        x = self.readout_layer(emb, batch)\n",
    "\n",
    "        for i in range(self.num_mlp_layers - 1):\n",
    "            x = self.mlps[i](x)\n",
    "            x = self.fc_nonlinear(x)\n",
    "            x = F.dropout(x, p=self.fc_dropout)\n",
    "\n",
    "        logits = self.mlps[-1](x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1edd49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readout을 identity로 하면 node classification이 되는듯\n",
    "# 논문 코드는 readout을 주로 max로 했는데, 나는 sum이 자연스럽지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c516ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0131]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0131]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0242]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = get_gnnNets(1, 1, {'gnn_latent_dim':[128,128,128]})\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                        [1, 0, 2, 1]], dtype=torch.long)\n",
    "edge_attr1 = torch.tensor([[0], [1], [1], [2]], dtype=torch.float)\n",
    "edge_attr2 = torch.tensor([[0], [0], [0], [0]], dtype=torch.float)\n",
    "\n",
    "\n",
    "data1 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr1)\n",
    "data11 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr1)\n",
    "data2 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr2)\n",
    "\n",
    "print(model(data1))\n",
    "print(model(data11))\n",
    "print(model(data2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514216d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
