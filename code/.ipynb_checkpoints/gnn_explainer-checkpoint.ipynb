{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3703718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from influenceDataset.ipynb\n",
      "importing Jupyter notebook from gnnNets.ipynb\n",
      "importing Jupyter notebook from base_explainer.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch_geometric.utils.loop import add_remaining_self_loops\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "from dig.version import debug\n",
    "from dig.xgraph.models.utils import subgraph\n",
    "#from dig.xgraph.method.utils import symmetric_edge_mask_indirect_graph\n",
    "from dig.xgraph.evaluation import XCollector\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "import import_ipynb\n",
    "from influenceDataset import get_dataloader, influenceDataset\n",
    "from gnnNets import get_gnnNets\n",
    "from base_explainer import ExplainerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b708bea0-0dce-4247-b406-4f849b79894d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\" \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07e13c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy_with_logit(y_pred: torch.Tensor, y_true: torch.Tensor, **kwargs):\n",
    "    return cross_entropy(y_pred, y_true.long(), **kwargs)\n",
    "\n",
    "\n",
    "class GNNExplainer(ExplainerBase):\n",
    "    r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "    Explanations for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "    structures and small subsets node features that play a crucial role in a\n",
    "    GNN’s node-predictions.\n",
    "    .. note:: For an example, see `benchmarks/xgraph\n",
    "        <https://github.com/divelab/DIG/tree/dig/benchmarks/xgraph>`_.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        explain_graph (bool, optional): Whether to explain graph classification model\n",
    "            (default: :obj:`False`)\n",
    "        indirect_graph_symmetric_weights (bool, optional): If `True`, then the explainer\n",
    "            will first realize whether this graph input has indirect edges, \n",
    "            then makes its edge weights symmetric. (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 epochs: int = 100,\n",
    "                 lr: float = 0.01,\n",
    "                 coff_edge_size: float = 0.001,\n",
    "                 coff_edge_ent: float = 0.001,\n",
    "                 coff_node_feat_size: float = 1.0,\n",
    "                 coff_node_feat_ent: float = 0.1,\n",
    "                 explain_graph: bool = False,\n",
    "                 indirect_graph_symmetric_weights: bool = False):\n",
    "        super(GNNExplainer, self).__init__(model, epochs, lr, explain_graph)\n",
    "        self.coff_node_feat_size = coff_node_feat_size\n",
    "        self.coff_node_feat_ent = coff_node_feat_ent\n",
    "        self.coff_edge_size = coff_edge_size\n",
    "        self.coff_edge_ent = coff_edge_ent\n",
    "        self._symmetric_edge_mask_indirect_graph: bool = indirect_graph_symmetric_weights   \n",
    "\n",
    "    \n",
    "    def __loss__(self, raw_preds: Tensor, x_label: Union[Tensor, int]):\n",
    "        loss = mse_loss(raw_preds, torch.tensor([[x_label]]).to(device))\n",
    "        \"\"\"\n",
    "        if self.explain_graph:\n",
    "            loss = cross_entropy_with_logit(raw_preds, x_label)\n",
    "        else:\n",
    "            loss = cross_entropy_with_logit(raw_preds[self.node_idx].reshape(1, -1), x_label)\n",
    "        \"\"\"\n",
    "        \n",
    "        m = self.edge_mask.sigmoid()\n",
    "        loss = loss + self.coff_edge_size * m.sum()\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coff_edge_ent * ent.mean()\n",
    "\n",
    "        if self.mask_features:\n",
    "            m = self.node_feat_mask.sigmoid()\n",
    "            loss = loss + self.coff_node_feat_size * m.sum()\n",
    "            ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "            loss = loss + self.coff_node_feat_ent * ent.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def gnn_explainer_alg(self,\n",
    "                          x: Tensor,\n",
    "                          edge_index: Tensor,\n",
    "                          edge_attr: Tensor,\n",
    "                          ex_label: Tensor,\n",
    "                          prediction,\n",
    "                          mask_features: bool = False,\n",
    "                          **kwargs\n",
    "                          ) -> Tensor:\n",
    "\n",
    "        # initialize a mask\n",
    "        self.to(x.device)\n",
    "        self.mask_features = mask_features\n",
    "\n",
    "        # train to get the mask\n",
    "        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask],\n",
    "                                     lr=self.lr)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "\n",
    "            if mask_features:\n",
    "                h = x * self.node_feat_mask.view(1, -1).sigmoid()\n",
    "            else:\n",
    "                h = x\n",
    "            raw_preds = self.model(x=h, edge_index=edge_index, edge_attr=edge_attr, **kwargs)\n",
    "            loss = self.__loss__(raw_preds, prediction)\n",
    "            if epoch % 20 == 0 and debug:\n",
    "                print(f'Loss:{loss.item()}')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), clip_value=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        return self.edge_mask.data\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, mask_features=False, target_label=None, prediction=None, **kwargs):\n",
    "        r\"\"\"\n",
    "        Run the explainer for a specific graph instance.\n",
    "        Args:\n",
    "            x (torch.Tensor): The graph instance's input node features.\n",
    "            edge_index (torch.Tensor): The graph instance's edge index.\n",
    "            mask_features (bool, optional): Whether to use feature mask. Not recommended.\n",
    "                (Default: :obj:`False`)\n",
    "            target_label (torch.Tensor, optional): if given then apply optimization only on this label\n",
    "            **kwargs (dict):\n",
    "                :obj:`node_idx` （int, list, tuple, torch.Tensor): The index of node that is pending to be explained.\n",
    "                (for node classification)\n",
    "                :obj:`sparsity` (float): The Sparsity we need to control to transform a\n",
    "                soft mask to a hard mask. (Default: :obj:`0.7`)\n",
    "                :obj:`num_classes` (int): The number of task's classes.\n",
    "        :rtype: (None, list, list)\n",
    "        .. note::\n",
    "            (None, edge_masks, related_predictions):\n",
    "            edge_masks is a list of edge-level explanation for each class;\n",
    "            related_predictions is a list of dictionary for each class\n",
    "            where each dictionary includes 4 type predicted probabilities.\n",
    "        \"\"\"\n",
    "        super().forward(x=x, edge_index=edge_index, edge_attr=edge_attr, **kwargs)\n",
    "        self.model.eval()\n",
    "\n",
    "        self_loop_edge_index, _ = add_remaining_self_loops(edge_index, num_nodes=self.num_nodes)\n",
    "\n",
    "        \"\"\"\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        # Get subgraph and relabel the node, mapping is the relabeled given node_idx.\n",
    "        if not self.explain_graph:\n",
    "            self.node_idx = node_idx = kwargs.get('node_idx')\n",
    "            assert node_idx is not None, 'An node explanation needs kwarg node_idx, but got None.'\n",
    "            if isinstance(node_idx, torch.Tensor) and not node_idx.dim():\n",
    "                node_idx = node_idx.to(self.device).flatten()\n",
    "            elif isinstance(node_idx, (int, list, tuple)):\n",
    "                node_idx = torch.tensor([node_idx], device=self.device, dtype=torch.int64).flatten()\n",
    "            else:\n",
    "                raise TypeError(f'node_idx should be in types of int, list, tuple, '\n",
    "                                f'or torch.Tensor, but got {type(node_idx)}')\n",
    "            self.subset, _, _, self.hard_edge_mask = subgraph(\n",
    "                node_idx, self.__num_hops__, self_loop_edge_index, relabel_nodes=True,\n",
    "                num_nodes=None, flow=self.__flow__())\n",
    "            self.new_node_idx = torch.where(self.subset == node_idx)[0]\n",
    "        \"\"\"\n",
    "\n",
    "        if kwargs.get('edge_masks'):\n",
    "            edge_masks = kwargs.pop('edge_masks')\n",
    "            self.__set_masks__(x, self_loop_edge_index)\n",
    "\n",
    "        else:\n",
    "            # Assume the mask we will predict\n",
    "            labels = tuple(i for i in range(kwargs.get('num_classes')))\n",
    "            ex_labels = tuple(torch.tensor([label]).to(self.device) for label in labels)\n",
    "\n",
    "            # Calculate mask\n",
    "            edge_masks = []\n",
    "            for ex_label in ex_labels:\n",
    "                if target_label is None or ex_label.item() == target_label.item():\n",
    "                    self.__clear_masks__()\n",
    "                    self.__set_masks__(x, self_loop_edge_index)\n",
    "                    edge_mask = self.gnn_explainer_alg(x, edge_index, edge_attr, ex_label, prediction).sigmoid()\n",
    "                    \n",
    "                    #if self._symmetric_edge_mask_indirect_graph:\n",
    "                    #    edge_mask = symmetric_edge_mask_indirect_graph(self_loop_edge_index, edge_mask)\n",
    "\n",
    "                    edge_masks.append(edge_mask)\n",
    "\n",
    "        hard_edge_masks = [self.control_sparsity(mask, sparsity=kwargs.get('sparsity')).sigmoid().to(self.device)\n",
    "                           for mask in edge_masks]  # 원래 코드에는 .sigmoid()가 없다. 그러나 없으면 오류나고 다른 method들에는 있길래 내가 추가 \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            related_preds = self.eval_related_pred(x, edge_index, edge_attr, hard_edge_masks, **kwargs)\n",
    "\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        return edge_masks, hard_edge_masks, related_preds\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acdd1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    dataset = influenceDataset('/data/URP','/data/URP/graphs')\n",
    "        \n",
    "    model = get_gnnNets(1, 1, {'gnn_latent_dim':[128,128,128], 'add_self_loop':False})  # 내가 다른 코드에서 add_self_loop를 지웠을 수 있다. 조심\n",
    "    model.load_state_dict(torch.load('/data/URP/model/bestmodel.pt'))\n",
    "    model.to(device) \n",
    "    \n",
    "    gnn_explainer = GNNExplainer(model,\n",
    "                                 epochs=100,\n",
    "                                 lr=0.01,\n",
    "                                 explain_graph=True)\n",
    "    gnn_explainer.device = device\n",
    "\n",
    "    index = 0\n",
    "    x_collector = XCollector()\n",
    "    for i, data in enumerate(dataset):\n",
    "        index += 1\n",
    "        data.edge_index, data.edge_attr = add_remaining_self_loops(data.edge_index, edge_attr=data.edge_attr, num_nodes=data.num_nodes)\n",
    "        data.to(device)\n",
    "        pred = model(data) \n",
    "        prediction = pred.argmax(-1).item()\n",
    "\n",
    "        edge_masks, hard_edge_masks, related_preds = \\\n",
    "            gnn_explainer(data.x, data.edge_index, data.edge_attr,\n",
    "                          sparsity=0.1,\n",
    "                          num_classes=dataset.num_classes,\n",
    "                         prediction=pred.item())\n",
    "        edge_masks = [edge_mask.to('cpu') for edge_mask in edge_masks]\n",
    "        x_collector.collect_data(hard_edge_masks, related_preds, label=prediction)\n",
    "\n",
    "    print(f'Fidelity: {x_collector.fidelity:.4f}\\n'\n",
    "          f'Fidelity_inv: {x_collector.fidelity_inv: .4f}\\n'\n",
    "          f'Sparsity: {x_collector.sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af4e4a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity: 228.0257\n",
      "Fidelity_inv:  0.0138\n",
      "Sparsity: 0.1000\n"
     ]
    }
   ],
   "source": [
    "pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "630a4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[{'zero': tensor([0.3027]), 'masked': tensor([360.6974]), 'maskout': tensor([360.6983]), 'origin': tensor([360.6983]), 'sparsity': tensor(0.5000)}]\n",
      "[tensor([1., 0., 0., 0., 0., 1., 1., 1., 0., 1.])]\n",
      "tensor([[360.6983]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "2\n",
      "[{'zero': tensor([0.3027]), 'masked': tensor([360.6974]), 'maskout': tensor([360.6982]), 'origin': tensor([360.6983]), 'sparsity': tensor(0.5000)}]\n",
      "[tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 1.])]\n",
      "tensor([[360.6983]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "3\n",
      "[{'zero': tensor([0.3027]), 'masked': tensor([360.6983]), 'maskout': tensor([360.6982]), 'origin': tensor([575.8438]), 'sparsity': tensor(0.5000)}]\n",
      "[tensor([1., 1., 1., 1., 0., 1., 0., 0., 0., 0.])]\n",
      "tensor([[575.8438]], grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "\"\"\"\n",
    "from torch_geometric.data import Data\n",
    "x = torch.tensor([[1], [0], [1], [1], [0]], dtype=torch.float)\n",
    "edge_index = torch.tensor([[0, 1, 2, 3, 3], [1, 2, 1, 1, 4]], dtype=torch.long)\n",
    "edge_attr1 = torch.tensor([[0], [0.8], [1], [0.5], [0.1]], dtype=torch.float)\n",
    "edge_attr2 = torch.tensor([[1], [0.2], [0], [0.8], [0.1]], dtype=torch.float)\n",
    "\n",
    "data11 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr1)\n",
    "data12 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr1)\n",
    "data21 = Data(x=x, edge_index=edge_index, edge_attr=edge_attr2)\n",
    "\n",
    "model = get_gnnNets(1, 1, {'gnn_latent_dim':[128,128,128], 'add_self_loop':False})\n",
    "model.load_state_dict(torch.load('../model/bestmodel.pt'))\n",
    "\n",
    "gnngi_explainer = GNNExplainer(model, epochs=10, lr=0.01, explain_graph=True)\n",
    "\n",
    "index = 0\n",
    "for i, data in enumerate([data11,data12,data21]):\n",
    "    index += 1\n",
    "    data.edge_index, data.edge_attr = add_remaining_self_loops(data.edge_index, edge_attr=data.edge_attr, num_nodes=data.num_nodes)\n",
    "    pred = model(data)\n",
    "    walks, edge_masks, related_preds = \\\n",
    "        gnngi_explainer(data.x, data.edge_index, data.edge_attr,\n",
    "                        sparsity=0.5,\n",
    "                        num_classes=1, prediction=pred.item())\n",
    "    print(index)\n",
    "    print(related_preds)\n",
    "    print(edge_masks)\n",
    "    print()\n",
    "\"\"\"\n",
    "# 같은 데이터로도 결과가 다른 것은 GNN에서 데이터마다 모델을 학습시킬 떄의 randomness때문인듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
