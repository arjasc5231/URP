{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eedf857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch_geometric.data.data import BaseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d55f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
    "class influenceDataset(Dataset):\n",
    "    def __init__(self, root, graph_dir):\n",
    "        self.graph_dir = graph_dir\n",
    "        super().__init__(root)\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):  # data_0.pt가 존재하면 이미 process했으므로 process()실행 안함\n",
    "        return ['data_0.pt']\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    여러 그래프를 처리할 수 있도록 process를 짰지만 현재 다른 파일(_350 등)이 추가되면서 작동 안함\n",
    "    \"\"\"\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        for graph_name in os.listdir(self.raw_dir):\n",
    "            edge_index = [[],[]]\n",
    "            edge_attr = []\n",
    "            with open(self.graph_dir+'/'+graph_name[:-7]+'.txt','r') as f:\n",
    "                n,m = map(int,f.readline().split())\n",
    "                for _ in range(m):\n",
    "                    u,v,p = f.readline().split()\n",
    "                    u,v,p = int(u),int(v),float(p)\n",
    "                    edge_index[0].append(u)\n",
    "                    edge_index[1].append(v)\n",
    "                    edge_attr.append([p])\n",
    "            edge_index = torch.tensor(edge_index)\n",
    "            edge_attr = torch.tensor(edge_attr)\n",
    "            \n",
    "            with gzip.open(self.raw_dir+'/'+graph_name, 'rb') as f: seeds,probs = pickle.load(f)\n",
    "            seeds = torch.from_numpy(np.expand_dims(seeds,axis=-1)).float()\n",
    "            probs = torch.from_numpy(np.sum(probs,axis=-1,keepdims=True)).float()\n",
    "            \n",
    "            for i in range(len(seeds)):\n",
    "                seed,prob = seeds[i],probs[i]\n",
    "                prob = torch.unsqueeze(prob, 0)  # 이렇게 해야 loss를 계산할 때 자연스럽다.\n",
    "                data = Data(x=seed, edge_index=edge_index, edge_attr=edge_attr, y=prob)\n",
    "                torch.save(data, os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "                idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return 347  # 10000개 처리하다 347개에서 중단함\n",
    "        #return len(os.listdir(self.processed_dir))-2\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset class가 (y의 데이터 개수)=y.size(0)이면 데이터의 unique 개수로 class 개수를 계산한다.\n",
    "    따라서 재귀에는 자연스럽게 사용할 수 없다.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def num_classes(self): return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d65764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size):\n",
    "    dataset = influenceDataset('/data/URP','/data/URP/graphs')\n",
    "    train_num = int(len(dataset)*0.8)\n",
    "    val_num = int(len(dataset)*0.1)\n",
    "    test_num = len(dataset)-train_num-val_num\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset,lengths=[train_num,val_num,test_num])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd24bb1-6e9b-4e7e-978c-f3717db91070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오는 작업은 나중에 raw생성할때로 통합, 파일로 저장하기\n",
    "def get_dataloader_10000_noprocessing(batch_size):\n",
    "    edge_index = [[],[]]\n",
    "    edge_attr = []\n",
    "    with open('/data/URP/graphs/Celebrity_train_JI.txt','r') as f:\n",
    "        n,m = map(int,f.readline().split())\n",
    "        for _ in range(m):\n",
    "            u,v,p = f.readline().split()\n",
    "            u,v,p = int(u),int(v),float(p)\n",
    "            edge_index[0].append(u)\n",
    "            edge_index[1].append(v)\n",
    "            edge_attr.append([p])\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_attr = torch.tensor(edge_attr)\n",
    "\n",
    "    with gzip.open('/data/URP/raw/Celebrity_train_JI.pkl.gz', 'rb') as f: seeds,probs = pickle.load(f)\n",
    "    seeds = torch.from_numpy(np.expand_dims(seeds,axis=-1)).float()\n",
    "    probs = torch.from_numpy(np.sum(probs,axis=-1,keepdims=True)).float()\n",
    "\n",
    "    datas = []\n",
    "    for i in range(len(seeds)):\n",
    "        seed,prob = seeds[i],probs[i]\n",
    "        prob = torch.unsqueeze(prob, 0)  # 이렇게 해야 loss를 계산할 때 자연스럽다.\n",
    "        data = Data(x=seed, edge_index=edge_index, edge_attr=edge_attr, y=prob)\n",
    "        datas.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_num = int(len(datas)*0.8)\n",
    "    val_num = int(len(datas)*0.1)\n",
    "    test_num = len(datas)-train_num-val_num\n",
    "    \n",
    "    train_dataloader = DataLoader(datas[:train_num], batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(datas[train_num:train_num+val_num], batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(datas[train_num+val_num:], batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c3a38-50eb-478a-9638-e822ccf21e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_350():\n",
    "    with gzip.open('Celebrity_train_JI_350.pkl.gz', 'rb') as f: datas = pickle.load(f)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9e40cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[156960, 1], edge_index=[2, 576780], edge_attr=[576780, 1], y=[20], batch=[156960], ptr=[21])\n",
      "20\n",
      "tensor([ 331.5699, 5738.3150, 7460.2946, 5248.2631, 2935.6470, 1930.2823,\n",
      "        1106.2486, 5479.7685,  845.4149, 4362.1423, 7679.0883,  855.9194,\n",
      "        5212.6871,  702.1163, 4452.2191, 5147.9294, 6752.9028, 5307.6616,\n",
      "        4554.9217, 4804.9743], dtype=torch.float64)\n",
      "DataBatch(x=[156960, 1], edge_index=[2, 576780], edge_attr=[576780, 1], y=[20], batch=[156960], ptr=[21])\n",
      "20\n",
      "tensor([5470.3094, 2750.6111, 5894.7667, 3520.1980, 7524.7225, 4587.5165,\n",
      "        1003.6427, 4570.4463, 3457.7579, 3942.4710, 7183.4812, 2528.2794,\n",
      "        6156.4984, 1894.2717, 4166.1275, 1239.1298,  755.1534, 4898.6982,\n",
      "        7141.5593, 6122.2534], dtype=torch.float64)\n",
      "DataBatch(x=[156960, 1], edge_index=[2, 576780], edge_attr=[576780, 1], y=[20], batch=[156960], ptr=[21])\n",
      "20\n",
      "tensor([7248.0818, 2074.6164, 1631.9656,  603.4873, 6324.7113, 7751.5885,\n",
      "        7505.9574, 7177.8660, 7158.8710, 3675.0412, 1772.8823, 1371.4469,\n",
      "        7085.8885, 5390.6488, 3515.0518, 2676.8399, 6368.4773, 1932.9200,\n",
      "        6728.0607, 2367.5558], dtype=torch.float64)\n",
      "DataBatch(x=[156960, 1], edge_index=[2, 576780], edge_attr=[576780, 1], y=[20], batch=[156960], ptr=[21])\n",
      "20\n",
      "tensor([2439.6871, 5299.7519, 5981.3269, 6923.5857,  225.1553, 5224.3310,\n",
      "        6058.5230, 6934.5225,  348.8382, 1808.3051, 3974.1594, 4139.8661,\n",
      "        5874.7345, 1358.8807, 7763.4198,  423.8559,   15.0107, 4332.1739,\n",
      "        1644.5466, 6554.3373], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\"\"\"\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.num_graphs)\n",
    "    print(batch.y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c5e5a0-a4f5-474c-b30e-683f4ca6cfba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataset = influenceDataset('/data/URP','/data/URP/graphs')\n",
    "print(len(dataset))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
